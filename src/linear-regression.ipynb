{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Como bem sabemos, este é um método criado por Francis Galton no século XIX, durante um estudo para determinar a relação entre a altura dos pais e seus filhos. O termo \"regressão\" foi usado pela primeira vez em seu artigo de 1886 \"Regressão à mediocridade na estatura hereditária\".\n",
    "\n",
    "Nesse estudo, ele percebeu que a altura do filho tende a ser semelhante a altura do pai, porém tenderá a média das alturas da população.\n",
    "\n",
    "Em uma regressão, estamos tentando traçar uma linha que seja o mais próxia o possível de todos os pontos.\n",
    "\n",
    "O método clássico é calcular os mínimos quadrados para obter a curva resultante de menor soma das distâncias quadradas de todos os pontos até ela.\n",
    "\n",
    "A Análise de Regressão Linear era considerada a principal técnica de modelagem estatística até meados do século XX. Seu principal objetivo é analisar a relação entre uma variável resposta e uma ou mais variáveis explicativas, para identificar uma função que a descreva.\n",
    "\n",
    "Através dela é possível entender as causas de variação de um fenômeno e predizer seu comportamento de acordo com as variáveis explicativas. Quando se tem apenas uma variável explicativa, a regressão é simples. Do contrário, tem-se uma regressão múltipla.\n",
    "\n",
    "Apesar de poderosa, a Análise de Regressão Linear exige fortes suposições para sua utilização, como normalidade, independência e homocedasticidade dos erros.\n",
    "\n",
    "Análise de regressão nos ajuda a encontrar respostas para:\n",
    "\n",
    "* Previsão de futuras observações.\n",
    "* Encontrar uma associação, relação entre variáveis.\n",
    "* Identifique quais variáveis contribuem mais para prever os resultados futuros.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teoria\n",
    "\n",
    "Vamos supor que tenhamos dados em tabela sobre duas variáveis: x e y. Se colocarmos cada par (x,y) em um gráfico, teremos uma figura como a seguinte:\n",
    "\n",
    "![01](img/scatter.png)\n",
    "\n",
    "O que o algoritmo de regressão linear faz é simplesmente achar a reta que melhor se encaixa entre os pontos:\n",
    "\n",
    "Assim, podemos prever (com erro) um valor de y dado um valor de x. Por exemplo, nós não temos uma observação em que x=1, mas gostaríamos de prever qual seria o valor de y caso x fosse 1. Basta então olhar na linha qual valor de y quando x assume o valor 1. Na imagem acima, y seria aproximadamente 2.5 (ponto amarelo).\n",
    "\n",
    "![02](img/scatter_line.png)\n",
    "\n",
    "Ok. Esse exemplo é bem simples e meramente ilustrativo. Suponha agora que y não dependa mais apenas de x, mas de x e z. Bem, nesse caso, teríamos um gráfico em 3D e a regressão linear acharia o plano que melhor se encaixa nos dados. E para mais dimensões? Digamos que y dependa de 100 variáveis. Nós não podemos mais visualizar esse caso, mas sabemos que não é muito diferente dos casos 2D ou 3D, só que agora a regressão linear acha o hiperplano que melhor se encaixa nos dados. \n",
    "\n",
    "### Justificativa matemática\n",
    "\n",
    "Imagine que temos dados em tabela, sendo que cada linha é uma observação e cada coluna uma variável. Então escolhemos uma das colunas para ser nossa variável dependente y (aquela que queremos prever) e as outras serão as variáveis independentes (X). Nosso objetivo é aprender como chegar das variáveis independentes na variável dependente, ou, em outras palavras, prever y a partir de X. Note que X é uma matriz nxd, em que n é o número de observações e d o número de dimensões; y é um vetor coluna nx1. Podemos definir o problema como um sistema de equações em que cada equação é uma observação:\n",
    "\n",
    "\\begin{cases}\n",
    "w_0 + w_1 x_1 + ... + w_d x_1 = y_1 \\\\\n",
    "w_0 + w_1 x_2 + ... + w_d x_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_n + ... + w_d x_n = y_n \\\\\n",
    "\\end{cases}\n",
    "\n",
    "Normalmente $n>d$, isto é, temos mais observações que dimensões. Sistemas assim costumam não ter solução, pois há muitas equações e poucas variáveis para ajustar. Intuitivamente, pense que, na prática, muitas coisas afetam a variável y, principalmente se ela for algo de interesse das ciências humanas como, por exemplo, preço, desemprego, felicidade etc. Muitas das coisas que afetam y não podem ser coletadas como dados; desse modo, as equações acima não teriam solução porque não teríamos todos os fatores que afetam y.\n",
    "\n",
    "Para lidar com esse problema, vamos adicionar nas equações um termo erro ε que representará os fatores que não conseguimos observar, erros de medição, etc.\n",
    "\n",
    "$$\\begin{cases}\n",
    "w_0 + w_1 x_{11} + ... + w_d x_{1d} + \\varepsilon_1 = y_1 \\\\\n",
    "w_0 + w_1 x_{21} + ... + w_d x_{2d} + \\varepsilon_2 = y_2 \\\\\n",
    "... \\\\\n",
    "w_0 + w_1 x_{n1} + ... + w_d x_{nd} + \\varepsilon_3 = y_n \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Ou, em forma de matriz:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & x_{11} & ... & x_{1d} \\\\\n",
    "1 & x_{21} & ... & x_{2d} \\\\\n",
    "\\vdots & \\vdots& \\vdots & \\vdots \\\\\n",
    "1 & x_{n1} & ... & x_{nd} \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_d \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "\\varepsilon_0 \\\\\n",
    "\\varepsilon_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\varepsilon_n \\\\\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_n \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Para estimar a equação acima, usaremos a técnica de Mínimos Quadrados Ordinários (MQO): queremos achar os $\\pmb{\\hat{w}}$ que minimizam os $n \\varepsilon^2$, ou, na forma de vetor, $\\pmb{\\epsilon}^T \\pmb{\\epsilon}$. Por que minimizar os erros quadrados? Assim como todo algoritmo de Aprendizado de Máquina, regressão linear também pode ser encarada como problemas de minimização de função custo. Então, nesse caso, nossa função custo é $L = \\pmb{\\epsilon}^T \\pmb{\\epsilon}$. Um nome comum dessa função é o custo quadrático L2, pois nesse caso o custo é o quadrado da norma L2 do vetor $\\pmb{\\epsilon}$. Note que nós poderíamos usar também a norma L1 do mesmo vetor como função custo. Ou ainda, poderíamos usar outras funções que adicionam uma penalidade também para o tamanho de $\\pmb{\\hat{w}}$, como acontece nos algoritmos de regressão [Ridge](https://en.wikipedia.org/wiki/Tikhonov_regularization), mas isso terá que ficar para outro tutorial. Por hora, a soma dos mínimos quadrados bastará como função custo, até porque ela tem a vantagem de deixar a matemática muito mais simples:\n",
    "\n",
    "$$\\pmb{\\epsilon}^T  \\pmb{\\epsilon} = (\\pmb{y} - \\pmb{\\hat{w}}X)^T(\\pmb{y} - \\pmb{\\hat{w}} X) \\\\= \\pmb{y}^T \\pmb{y} - \\pmb{\\hat{w}}^T X^T \\pmb{y} - \\pmb{y}^T X \\pmb{\\hat{w}} + \\pmb{\\hat{w}} X^T X \\pmb{\\hat{w}} \\\\= \\pmb{y}^T \\pmb{y} - 2\\pmb{\\hat{w}}^T X^T \\pmb{y} + \\pmb{\\hat{w}} X^T X \\pmb{\\hat{w}}$$\n",
    "\n",
    "Aqui, usamos o fato de que $\\pmb{\\hat{w}}^T X^T \\pmb{y}$ e $\\pmb{y}^T X \\pmb{\\hat{w}}$ são simplesmente escalares $1x1$ e a transposta de um escalar é o mesmo escalar: $\\pmb{\\hat{w}}^T X^T \\pmb{y} = (\\pmb{\\hat{w}}^T X^T \\pmb{y})^T = \\pmb{y}^T X \\pmb{\\hat{w}}$. Derivando em $\\pmb{\\hat{w}}$ e achando a CPO:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
